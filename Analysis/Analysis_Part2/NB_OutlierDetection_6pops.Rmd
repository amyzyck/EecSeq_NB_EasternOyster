---
title: "NB_Outlier_Detection_6pops"
author: "Amy Zyck"
date: '2023-03-12'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Outlier Detection programs performed on filtered SNP data set. Filtering steps are documented [here](https://github.com/amyzyck/EecSeq_NB_EasternOyster/blob/master/Analysis/Analysis_Part2/EecSeq_Cvirginica_Filtering_Part2.md). Four detecion programs are used: PCAdapt, Outflank, Bayescan, and LFMM. Identified outliers are saved to a .txt file and combined to create an outlier SNP data set for Population and Seascape Genomics Analyses. 

The first part of PCAdapt is using a VCF file for all 8 populations: `SNP.TRSBOTHdp5g5mafMIp9g9dnDNAmaf052A8.recode.vcf`. It's determined that two populations within this VCF file are likely not wild populations and are therefore removed before repeating PCAdapt and the remaining 3 detection programs.  

# PCAdapt

```{r}
# install and load pcadapt
#install.packages("pcadapt")
library(pcadapt)
```

```{r}
# Convert VCF file to pcadapt format

vcf2pcadapt("SNP.TRSBOTHdp5g5mafMIp9g9dnDNAmaf052A8.recode.vcf", output = "tmp.pcadapt1", allele.sep = c("/", "|"))

filename <- read.pcadapt("tmp.pcadapt1", type = "pcadapt")
```

```{r}
# Choosing the number K of Principal Components
# We start off with a large number of PCs
x1 <- pcadapt(input = filename, K = 20)
```

```{r}
# Plot the likelihoods using a screeplot
plot(x1, option = "screeplot")
```

```{r}
# Plotting likelihoods again, but with 10 Principal Components
plot(x1, option = "screeplot", K = 10)
```

```{r}
# Create population designations
poplist.names1 <- c(rep("BAR", 10),rep("BIS", 10),rep("GB", 10),rep("GHP", 10), rep("KIC", 10),rep("MCD", 10),rep("NAR", 10), rep("PVD", 10))
```

```{r}
# Score Plot
# Plot the first two PCs
plot(x1, option = "scores", pop = poplist.names1)
```

```{r}
plot(x1)
```


## NAR and GHP removed File `SNP.TRSBOTHdp5g5mafMIp9g9dnDNAmaf052A6.recode.vcf`

```{r}
# Convert VCF file to pcadapt format

vcf2pcadapt("SNP.TRSBOTHdp5g5mafMIp9g9dnDNAmaf052A6.recode.vcf", output = "tmp.pcadapt6", allele.sep = c("/", "|"))

filename6 <- read.pcadapt("tmp.pcadapt6", type = "pcadapt")
```

```{r}
# Choosing the number K of Principal Components
# We start off with a large number of PCs
x6 <- pcadapt(input = filename6, K = 20)
```

```{r}
# Plot the likelihoods using a screeplot
plot(x6, option = "screeplot")
```

```{r}
# Plotting likelihoods again, but with 10 Principal Components
plot(x6, option = "screeplot", K = 10)
```

```{r}
# Create population designations minus NAR, NAR2, GHP
poplist.names6 <- c(rep("BAR", 10),rep("BIS", 10),rep("GB", 10), rep("KIC", 10),rep("MCD", 10), rep("PVD", 10))
```

```{r}
# Score Plot
# Plot the first two PCs
plot(x6, option = "scores", pop = poplist.names6)
```

```{r}
# PCA plot with PCs 3 and 4
plot(x6, option = "scores", i = 3, j = 4, pop = poplist.names6)
```

```{r}
#evaluating LD in dataset - looking to see if loading are clustered in certain genomic regions (following https://bcm-uga.github.io/pcadapt/articles/pcadapt.html#f--linkage-disequilibrium-ld-thinning)
par(mfrow = c(2, 2))
for (i in 1:4)
  plot(x6$loadings[, i], pch = 19, cex = .3, ylab = paste0("Loadings PC", i))
```
Top left, PC1, is determined by one (maybe 2) genomic regions, also likley still an inversion.

```{r}
# performing SNP thinning in order to compute PCs
res6 <- pcadapt(filename6, K = 20, LD.clumping = list(size = 5000, thr = 0.05))
plot(res6, option = "screeplot")
```

```{r}
# Score Plot
# Plot the first two PCs
plot(res6, option = "scores", pop = poplist.names6)
```


```{r}
# PCA plot with PCs 3 and 4
plot(res6, option = "scores", i = 3, j = 4, pop = poplist.names6)
```

```{r}
res6 <- pcadapt(filename6, K = 2, LD.clumping = list(size = 5000, thr = 0.05))
par(mfrow = c(1,1))
for (i in 1)
  plot(res6$loadings[, i], pch = 19, cex = .3, ylab = paste0("Loadings PC", i))
```

```{r}
plot(res6)
```

```{r}
res6$singular.values
```

```{r}
# Create Q-Q Plot
plot(res6, option = "qqplot", threshold = 0.1)
```

```{r}
# Look at p-value distribution 
plot(res6, option = "stat.distribution")
```

#### Assigning an alpha value

```{r}
library(qvalue) # transforms p-values into q-values.
```

```{r}
qval6 <- qvalue(res6$pvalues)$qvalues
alpha6 <- 0.1 # expected false discovery rate lower than 1% - raised from 0.05
```

#### Save Outliers

```{r}
outliers6 <- which(qval6 < alpha6)
outliers6
```

```{r}
# Save outliers to .txt file 
invisible(lapply(outliers6, write, "outliers_pcadapt_thinned.txt", append=TRUE))
```

# OutFLANK

```{r}
# Load all necessary packages
library(OutFLANK)  # outflank package
library(vcfR)
library(bigsnpr)   # package for Linkage Disequilibrium pruning
```

```{r}
# Convert VCF to OutFLANK format
my_vcf6 <- read.vcfR("SNP.TRSBOTHdp5g5mafMIp9g9dnDNAmaf052A6.recode.vcf")

geno6 <- extract.gt(my_vcf6) # Character matrix containing the genotypes
position6 <- getPOS(my_vcf6) # Positions in bp
chromosome6 <- getCHROM(my_vcf6) # Chromosome information

G6 <- matrix(NA, nrow = nrow(geno6), ncol = ncol(geno6))

G6[geno6 %in% c("0/0", "0|0")] <- 0
G6[geno6  %in% c("0/1", "1/0", "1|0", "0|1")] <- 1
G6[geno6 %in% c("1/1", "1|1")] <- 2

# NA should be replaced with “9” to work with the functions in the OutFLANK package
G6[is.na(G6)] <- 9

head(G6[,1:10])
```

```{r}
# Convert pop designations to a vector
pop6 <- as.vector(poplist.names6)
pop6
```

```{r}
# Calculate Fst
my_fst6 <- MakeDiploidFSTMat(t(G6), locusNames = paste0(chromosome6,"_", position6), popNames = pop6)

head(my_fst6)
# the output will be a table showing Fst statistics for each locus
```

```{r}
# Plot heterozygosity vs Fst
plot(my_fst6$He, my_fst6$FST)
```

```{r}
# Plot Fst vs FstNoCorr
plot(my_fst6$FST, my_fst6$FSTNoCorr)
abline(0,1)
```

### We need to give OUTFlank a set of quasi-independent SNPs to estimate the neutral FST distribution.

```{r}
# Load additional packages
library("dplyr")
library("stringr")
```

```{r}
# Chromosomes need to be of class integer for this to work.
# Visualizing chromosomes
chrom_unique6 <- unique(chromosome6)
print(chrom_unique6)
```

```{r}
# Removing "NC_" from the chromosome name so it can be converted to an integer
chrom_new6 <- chromosome6%>%str_replace("NC_","")

# Converting the character string into an integer
chrom6 <- as.integer(chrom_new6)
```

```{r}
#chromosome and position need to be sorted for imputation to work.
chrom_sort6 <- sort(chrom6)
pos_sort6 <- sort(position6)
```

### Note: This filtering program does not allow for missing genotype values.

Using `snp_fastImputeSimple` to impute missing genotypes

```{r}
# Converting the genotype matrix to class FBM.code256
G1_6 <- add_code256(big_copy(t(G6),type="raw"),code=bigsnpr:::CODE_012)

# Imputing missing genotypes
G_missSimple6 <- snp_fastImputeSimple(G1_6,method = "mode")
```

```{r}
newpc6 <-snp_autoSVD(G_missSimple6,infos.chr =chrom_sort6,infos.pos = pos_sort6)
which_pruned6 <- attr(newpc6, which="subset") # Indexes of remaining SNPS after pruning
length(which_pruned6)
```

### Run the OutFLANK() function to estimate the parameters on the neutral FST distribution.

```{r}
out_trim6 <- OutFLANK(my_fst6[which_pruned6,], NumberOfSamples=60, qthreshold = 0.05, Hmin = 0.1)
str(out_trim6)
```

```{r}
# Check the fit and make sure it looks good, especially in the right tail:
OutFLANKResultsPlotter(out_trim6, withOutliers = TRUE, NoCorr = TRUE, Hmin = 0.1,binwidth = 0.001, Zoom = FALSE, RightZoomFraction = 0.05, titletext = NULL)
```

```{r}
# Check the p-value histogram
hist(out_trim6$results$pvaluesRightTail)
```

```{r}
# Using estimated neutral mean FST and df to calculate P-values for all loci
P1_6 <- pOutlierFinderChiSqNoCorr(my_fst6, Fstbar = out_trim6$FSTNoCorrbar, dfInferred = out_trim6$dfInferred, qthreshold = 0.05, Hmin=0.1)
```

```{r}
# Identify outliers SNPs
my_out6 <- P1_6$OutlierFlag==TRUE
plot(P1_6$He, P1_6$FST, pch=19, col=rgb(0,0,0,0.1))
points(P1_6$He[my_out6], P1_6$FST[my_out6], col="blue")
```

```{r}
P1_6[which(P1_6$OutlierFlag == TRUE),]

outliers_out6 <- P1_6[which(P1_6$OutlierFlag == TRUE),]
outliers_list6 <- outliers_out6$LocusName
outliers_list6
```

No outliers detected

```{r}
invisible(lapply(outliers_list6, write, "outliers_list_outflank.txt", append=TRUE))
```


# BayeScan

> BayeScan aims at identifying candidate loci under natural selection from genetic data, using differences in allele frequencies between populations. BayeScan is based on the multinomial-Dirichlet model. One of the simplest possible scenarios covered consists of an island model in which subpopulation allele frequencies are correlated through a common migrant gene pool from which they differ in varying degrees. The difference in allele frequency between this common gene pool and each subpopulation is measured by a subpopulation specific FST coefficient. Therefore, this formulation can consider realistic ecological scenarios where the effective size and the immigration rate may differ among subpopulations.

**_Note_**: PGDSpider2 does not run on Python version 3.11.0, which is the version in my current conda environment. Therefore, I am creating and activating a new conda environment with a previous version of Python that I know PGDSpider2 works with. You will also need to check the Java version. This works best with Java 8.

## _In Terminal_

```
$ conda create -n nb_capture4 python=3.7.3
$ conda activate nb_capture4
$ conda install openjdk=8
```

**Convert VCF file to other outputs**

```
# load in configuration file that will convert VCF to BayeScan format
$ curl -L -O https://raw.githubusercontent.com/amyzyck/EecSeq_NB_EasternOyster/master/Scripts/BSsnp.spid
$ chmod +x BSsnp.spid

$ java -jar /usr/local/bin/PGDSpider2-cli.jar -spid BSsnp.spid -inputfile SNP.TRSBOTHdp5g5mafMIp9g9dnDNAmaf052A8.recode.vcf -outputfile SNP8.BS

output:
WARN  16:43:13 - PGDSpider configuration file not found! Loading default configuration.
initialize convert process...
read input file...
read input file done.
write output file...
write output file done.
```

**Run BayeScan**

```
$ BayeScan2.1_linux64bits SNP8.BS -thin 50 -nbp 30
```

**Copy R source file**

This file is used to plot figures for the software Bayescan in R.

```
$ curl -L -O https://raw.githubusercontent.com/z0on/2bRAD_denovo/master/plot_R.r
$ chmod +x plot_R.r

## Remaining Bayescan steps are completed in RStudio:

### Plotting outliers

```{r}
source("plot_R.r")
plot_bayescan("SNP6_fst.txt")
```

```{r}
# Identifying outliers using a False discovery rate of 10%.
bs6 <- plot_bayescan("SNP6_fst.txt", FDR = 0.1)
```

```{r}
bs6$outliers
```

No outliers detected

```{r}
# Save outliers to .txt file
invisible(lapply(bs6, write, "outliers_bayescan.txt", append=TRUE))
```

# LFMM

Following steps documented by pgugger [here](https://github.com/pgugger/LandscapeGenomics/blob/master/2019/Exercise3.md)

# loading the required libraries

```{r}
#if (!require("BiocManager", quietly = TRUE))
#install.packages("BiocManager")

#BiocManager::install("LEA")
```

```{r}
library(LEA)
```

```{r}
# run snmf to estimate K, considering K from 1-4:
project_6pops = NULL
project_6pops = snmf("snp_6.lfmm", K = 1:10, entropy = TRUE, repetitions = 10, project = "new")
pdf("sNMF_6pops.pdf")
plot(project_6pops, col = "blue", pch = 19, cex = 1.2)
dev.off()
```

```{r}
best_6 = which.min(cross.entropy(project_6pops, K = 2))
pdf("sNMF.barchart_6pops.pdf")
barchart(project_6pops, K = 2, run = best_6, border = NA, space = 0, col = c("red", "blue"), xlab = "Individuals", ylab = "Ancestry proportions") -> bp_6
axis(1, at = 1:length(bp_6$order), labels = bp_6$order, las=1, cex.axis = .3)
dev.off()
```

```{r}
#imputing any missing data
project.missing_6pops = snmf("snp_6.lfmm", K = 1,
        entropy = TRUE, repetitions = 10,
        project = "new")
```

```{r}
# select the run with the lowest cross-entropy value
best_6 = which.min(cross.entropy(project.missing_6pops, K = 1))
# Impute the missing genotypes
impute(project.missing_6pops, "snp_6.lfmm",
       method = 'mode', K = 1, run = best_6)
## Missing genotype imputation for K = 1
## Missing genotype imputation for run = 6
## Results are written in the file:  snp_.lfmm_imputed.lfmm
# Proportion of correct imputation results
dat.imp_6 = read.lfmm("snp_6.lfmm_imputed.lfmm")
#mean( tutorial.R[dat == 9] == dat.imp[dat == 9] )
```

```{r}
# Prepping the environmental data
clim.env_6pops <- strata_6pops[,5:9]
colnames(clim.env_6pops) <- NULL
row.names(clim.env_6pops) <- NULL
clim.env_6pops
write.table(clim.env_6pops, "~/NB_capture_both/NB_ddhaplo/NB_OutlierDetect_both/NB_OutlierDetect_6pops/clim_6pops.env", sep="\t", quote=F, row.names=F)
```

```{r}
#run lfmm
project_6pops = NULL
project_6pops = lfmm("snp_6.lfmm_imputed.lfmm", "clim_6pops.env", K = 1, repetitions = 5, CPU = 16, iterations = 1000, burnin = 500, project = "new")
```

```{r}
#combine the data from the three repetitions and compute new calibrated P-values
#first extract the z-scores for all repetitions for a given climate variable (in this case sewage effluent), then take the median
z.se_6pops = z.scores(project_6pops, K = 1, d = 1)
z.se_6pops <- apply(z.se_6pops, 1, median)

# calculate λ (the "genomic inflation factor"), which is commonly used for calibration of P-values
lambda.se_6pops = median(z.se_6pops^2)/qchisq(0.5, df = 1)
lambda.se_6pops

#The calibrated or "adjusted" P-values are then calculated as follows:
p.se.adj_6pops = pchisq(z.se_6pops^2/lambda.se_6pops, df = 1, lower = FALSE)
```

```{r}
# repeat this correction procedure with the other four climate variables

#temp
z.temp_6pops = z.scores(project_6pops, K = 1, d = 2)
z.temp_6pops <- apply(z.temp_6pops, 1, median)

lambda.temp_6pops = median(z.temp_6pops^2)/qchisq(0.5, df = 1)
lambda.temp_6pops

p.temp.adj_6pops = pchisq(z.temp_6pops^2/lambda.temp_6pops, df = 1, lower = FALSE)

#salinity
z.sal_6pops = z.scores(project_6pops, K = 1, d = 3)
z.sal_6pops <- apply(z.sal_6pops, 1, median)

lambda.sal_6pops = median(z.sal_6pops^2)/qchisq(0.5, df = 1)
lambda.sal_6pops

p.sal.adj_6pops = pchisq(z.sal_6pops^2/lambda.sal_6pops, df = 1, lower = FALSE)

#pH
z.ph_6pops = z.scores(project_6pops, K = 1, d = 4)
z.ph_6pops <- apply(z.ph_6pops, 1, median)

lambda.ph_6pops = median(z.ph_6pops^2)/qchisq(0.5, df = 1)
lambda.ph_6pops

p.ph.adj_6pops = pchisq(z.ph_6pops^2/lambda.ph_6pops, df = 1, lower = FALSE)

#DO
z.do_6pops = z.scores(project_6pops, K = 1, d = 5)
z.do_6pops <- apply(z.do_6pops, 1, median)

lambda.do_6pops = median(z.do_6pops^2)/qchisq(0.5, df = 1)
lambda.do_6pops

p.do.adj_6pops = pchisq(z.do_6pops^2/lambda.do_6pops, df = 1, lower = FALSE)
```

```{r}
# investigating chosen K value and adjusted p-values using histogram
pdf("LFMM_P_Histograms_6pops.pdf")
par(mfrow = c(5,1))
hist(p.se.adj_6pops, col = "blue", main = "Sewage Effluent", xlab='')
hist(p.temp.adj_6pops, col = "blue", main = "Temperature", xlab='')
hist(p.sal.adj_6pops, col = "blue", main = "Salinity", xlab='')
hist(p.ph.adj_6pops, col = "blue", main = "pH", xlab='')
hist(p.do.adj_6pops, col = "blue", main = "Dissolved oxygen", xlab='')
dev.off()
```

```{r}
#Adjusting p-values to q-values to correct for multiple testing - running so many statistical tests can result in many tests appearing significant by chance
library(qvalue)
q.se_6pops<-qvalue(p.se.adj_6pops)$qvalues
q.temp_6pops<-qvalue(p.temp.adj_6pops)$qvalues
q.sal_6pops<-qvalue(p.sal.adj_6pops)$qvalues
q.ph_6pops<-qvalue(p.ph.adj_6pops)$qvalues
q.do_6pops<-qvalue(p.do.adj_6pops)$qvalues
```

```{r}
#Comparing the number of significant tests based on P-values versus Q-values

#SE - p=3395, q=59
sum(p.se.adj_6pops<0.05)
sum(q.se_6pops<0.05)

#Temp - p=3395, q=58 
sum(p.temp.adj_6pops<0.05)
sum(q.temp_6pops<0.05)

#Salinity - p=3572, q=34
sum(p.sal.adj_6pops<0.05)
sum(q.sal_6pops<0.05)

#pH - p=3551, q=75
sum(p.ph.adj_6pops<0.05)
sum(q.ph_6pops<0.05)

#DO - p=3556, q=53
sum(p.do.adj_6pops<0.05)
sum(q.do_6pops<0.05)
```

```{r}
#Manhatten plot to visually summarize large number of association tests
#I'm breaking it up into 5 separate plots so I can view them more easily
pdf("LFMM_Manhattan_SE_6pops.pdf")
par(mfrow = c(1,1))
plot(-log10(q.se_6pops), pch = 19, col = "blue", cex = .7, xlab = '')
dev.off()

pdf("LFMM_Manhattan_Temp_6pops.pdf")
par(mfrow = c(1,1))
plot(-log10(q.temp_6pops), pch = 19, col = "blue", cex = .7, xlab = '')
dev.off()

pdf("LFMM_Manhattan_Sal_6pops.pdf")
par(mfrow = c(1,1))
plot(-log10(q.sal_6pops), pch = 19, col = "blue", cex = .7, xlab = '')
dev.off()

pdf("LFMM_Manhattan_pH_6pops.pdf")
par(mfrow = c(1,1))
plot(-log10(q.ph_6pops), pch = 19, col = "blue", cex = .7, xlab = '')
dev.off()

pdf("LFMM_Manhattan_DO_6pops.pdf")
par(mfrow = c(1,1))
plot(-log10(q.do_6pops), pch = 19, col = "blue", cex = .7, xlab = '')
dev.off()
```

```{r}
# looking at the set of significant SNPS that also have very high or low z-scores

#se
sum(q.se_6pops<0.01 & abs(z.se_6pops)>2.5)

#temp
sum(q.temp_6pops<0.01 & abs(z.temp_6pops)>2.5)

#sal
sum(q.sal_6pops<0.01 & abs(z.sal_6pops)>2.5)

#pH
sum(q.ph_6pops<0.01 & abs(z.ph_6pops)>2.5)

#DO
sum(q.do_6pops<0.01 & abs(z.do_6pops)>2.5)
```

```{r}
# Here I am looking for SNPs that have a significant relationship with multiple climate variable. I'm checking out each env variable combo. Only sal+ph result in a shared SNP.
sum(q.se_6pops<0.01 & abs(z.se_6pops)>2.5 & q.temp_6pops<0.01 & abs(z.temp_6pops)>2.5) 
sum(q.se_6pops<0.01 & abs(z.se_6pops)>2.5 & q.sal_6pops<0.01 & abs(z.sal_6pops)>2.5) 
sum(q.se_6pops<0.01 & abs(z.se_6pops)>2.5 & q.ph_6pops<0.01 & abs(z.ph_6pops)>2.5) 
sum(q.se_6pops<0.01 & abs(z.se_6pops)>2.5 & q.do_6pops<0.01 & abs(z.do_6pops)>2.5) 
sum(q.temp_6pops<0.01 & abs(z.temp_6pops)>2.5 & q.sal_6pops<0.01 & abs(z.sal_6pops)>2.5) 
sum(q.temp_6pops<0.01 & abs(z.temp_6pops)>2.5 & q.ph_6pops<0.01 & abs(z.ph_6pops)>2.5) 
sum(q.temp_6pops<0.01 & abs(z.temp_6pops)>2.5 & q.do_6pops<0.01 & abs(z.do_6pops)>2.5) 
sum(q.sal_6pops<0.01 & abs(z.sal_6pops)>2.5 & q.ph_6pops<0.01 & abs(z.ph_6pops)>2.5) 
sum(q.sal_6pops<0.01 & abs(z.sal_6pops)>2.5 & q.do_6pops<0.01 & abs(z.do_6pops)>2.5) 
sum(q.ph_6pops<0.01 & abs(z.ph_6pops)>2.5 & q.do_6pops<0.01 & abs(z.do_6pops)>2.5)
```

```{r}
# Combining all z and Q-values into a single table 
lfmm_6pops.results <- cbind(z.se_6pops, q.se_6pops, z.temp_6pops, q.temp_6pops, z.sal_6pops, q.sal_6pops, z.ph_6pops, q.ph_6pops, z.do_6pops, q.do_6pops)
head(lfmm_6pops.results)  #Note that the SNP locus numbers and positions are absent.
```

```{r}
snp_6pops.names <-read.table("./snp_6.012.pos", header=F)
colnames(snp_6pops.names) <- c("locus", "position")
lfmm_6pops.results <- cbind(snp_6pops.names, lfmm_6pops.results)
head(lfmm_6pops.results)  #Now we have a clear table with locus names and LFMM results

#This generates a file with all SNP info and z and Q-values saved. I would then go through this to identify the putatively adaptive SNPs. 
write.table(lfmm_6pops.results, "lfmm_6pops.results", sep="\t", quote=F, row.names=F)
```

```{r}
# Now I'm going through the lfmm.results file and locating the significant SNPs identified above. I will do this for each environmental variable and save to an outliers list. I am also going to do this for the SNP shared with pH and Sal. All will be combined at the end. 

#Significant relationship with one specific climate variable:
#SE (should be 3)
se_outlier_list_6pops <- lfmm_6pops.results[ which(lfmm_6pops.results$q.se_6pops < 0.01 & abs(lfmm_6pops.results$z.se_6pops) > 2.5), ]
#Temp (should be 3)
temp_outlier_list_6pops <- lfmm_6pops.results[ which(lfmm_6pops.results$q.temp_6pops < 0.01 & abs(lfmm_6pops.results$z.temp_6pops) > 2.5), ]
#Sal (should be 6)
sal_outlier_list_6pops <- lfmm_6pops.results[ which(lfmm_6pops.results$q.sal_6pops < 0.01 & abs(lfmm_6pops.results$z.sal_6pops) > 2.5), ]
#pH (should be 8)
ph_outlier_list_6pops <- lfmm_6pops.results[ which(lfmm_6pops.results$q.ph_6pops < 0.01 & abs(lfmm_6pops.results$z.ph_6pops) > 2.5), ]
#DO (should be 16)
do_outlier_list_6pops <- lfmm_6pops.results[ which(lfmm_6pops.results$q.do_6pops < 0.01 & abs(lfmm_6pops.results$z.do_6pops) > 2.5), ]
```

```{r}
# This is a bit complicated, but I'm saving the row names (snp numbers) as a column in the dataframe then extracting those values from the table to save to a .txt file 

#SE
se_outlier_list1_6pops <- se_outlier_list_6pops
se_outlier_list1_6pops$snps <- row.names(se_outlier_list1_6pops)
se_outliers_6pops <- se_outlier_list1_6pops$snps

invisible(lapply(se_outliers_6pops, write, "se_outliers_lfmm_6pops.txt", append=TRUE))

#Temp
temp_outlier_list1_6pops <- temp_outlier_list_6pops
temp_outlier_list1_6pops$snps <- row.names(temp_outlier_list1_6pops)
temp_outliers_6pops <- temp_outlier_list1_6pops$snps

invisible(lapply(temp_outliers_6pops, write, "temp_outliers_lfmm_6pops.txt", append=TRUE))

#Salinity
sal_outlier_list1_6pops <- sal_outlier_list_6pops
sal_outlier_list1_6pops$snps <- row.names(sal_outlier_list1_6pops)
sal_outliers_6pops <- sal_outlier_list1_6pops$snps

invisible(lapply(sal_outliers_6pops, write, "sal_outliers_lfmm_6pops.txt", append=TRUE))

#pH
ph_outlier_list1_6pops <- ph_outlier_list_6pops
ph_outlier_list1_6pops$snps <- row.names(ph_outlier_list1_6pops)
ph_outliers_6pops <- ph_outlier_list1_6pops$snps

invisible(lapply(ph_outliers_6pops, write, "ph_outliers_lfmm_6pops.txt", append=TRUE))

#DO
do_outlier_list1_6pops <- do_outlier_list_6pops
do_outlier_list1_6pops$snps <- row.names(do_outlier_list1_6pops)
do_outliers_6pops <- do_outlier_list1_6pops$snps

invisible(lapply(do_outliers_6pops, write, "do_outliers_lfmm_6pops.txt", append=TRUE))
```


# LFMM ridge 

I want to run lfmm a different way to see how results compare. I am following steps documented by B.R. Forester [here](https://bookdown.org/hhwagner1/LandGenCourse_book/WE_11.html). 


```{r}
# if(!requireNamespace("qvalue", quietly = TRUE)) {  
# if (!requireNamespace("BiocManager", quietly = TRUE))
# install.packages("BiocManager")
# BiocManager::install(version = "3.14")
# BiocManager::install("qvalue")
# }
# if(!requireNamespace("lfmm", quietly = TRUE)) {  
#  remotes::install_github("bcm-uga/lfmm")
# }
```

```{r}
library(vegan)    # Used to run PCA & RDA
library(lfmm)     # Used to run LFMM
library(qvalue)   # Used to post-process LFMM output
library(vcfR)
```


# Import the genetic data 

```{r}
# to make it tab separated - sed 's/ \+/\t/g' snp_6.lfmm_imputed.lfmm  > snp_6.lfmm_imputed_tab.lfmm
gen_6<-read.delim("snp_6.lfmm_imputed_tab.lfmm",header = FALSE)
```

```{r}
#LFMM requires a complete dataframe. I am using the snp_6.lfmm_imputed_tab.lfmm file created for running lfmm above, which imputes missing data based on the most common genotype for each SNP. I will move forward with this and see what happens
sum(is.na(gen_6))
```

```{r}
env_6pops.lfmm <- strata_6pops
str(env_6pops.lfmm)
```

```{r}
#changing individual names and Populations to characters (can't be factors)
env_6pops.lfmm$Individual <- as.character(env_6pops.lfmm$Individual)
```

```{r}
#subsetting the environmental dataset to just include the last 5 variables and then shortening their names
pred_6 <- env_6pops.lfmm[,5:9]
colnames(pred_6) <- c("SE","Temp","Sal","pH","DO")
```

```{r}
# Running a PCA on the environmental variables. We can use the first PC as a synthetic predictor
pred_6.pca <- rda(pred_6, scale=T)
summary(pred_6.pca)$cont
```

```{r}
screeplot(pred_6.pca, main = "Screeplot: Eigenvalues of Oyster Predictor Variables")
```

```{r}
## correlations between the PC axis and predictors:
round(scores(pred_6.pca, choices=1:5, display="species", scaling=0), digits=3)
```

46% of the variance in the predictors is explained by the first PC axis, and 27% by the second axis. We could follow up with an LFMM model using the second axis as a predictor, if we wanted. The strongest correlations with PC1 are temperature (Temp), Sewage Effluent (SE), Dissolved Oxygen (DO) and Salinity (Sal).

We’ll store our synthetic PC axis predictor as pred.PC1 for use in LFMM.

```{r}
pred_6.PC1 <- scores(pred_6.pca, choices=1, display="sites", scaling=0)
```

## Determine K (estimate of number of populations in the data)

```{r}
#Using broken stick criterion to determine k - PCs should be retained as long as observed eigenvalues are higher than corresponding random broken stick components 

#Testing it out on the environmental data
screeplot(pred_6.pca, main = "Screeplot of Oyster Predictor Variables with Broken Stick", bstick=TRUE, type="barplot")
```
PC1 and PC2 explain more than the random broken stick components, while PC3 + do not. If this were genomic data, and we were determining a value of K using this approach, we’d set K = 3.

```{r}
# Now looking at the genetic data
gen_6.pca <- rda(gen_6, scale=T)
screeplot(gen_6.pca, main = "Screeplot of Genetic Data with Broken Stick", bstick=TRUE, type="barplot")
```

For the genomic data, we can see that none of the PCs have eigenvalues greater than random (greater than the broken stick values in red). This effectively means that K=1 for the genomic data set, based on a PCA assessment. 

```{r}
K <- 1
```

## Run LFMM

```{r}
oys_6.lfmm <- lfmm_ridge(Y=gen_6, X=pred_6.PC1, K=K) ## change K as you see fit
```

```{r}
#calculating test statistics for the predictor
oys_6.pv <- lfmm_test(Y=gen_6, X=pred_6.PC1, lfmm=oys_6.lfmm, calibrate="gif")

names(oys_6.pv) # this object includes raw z-scores and p-values, as well as GIF-calibrated scores and p-values
```

```{r}
#Looking at the genomic inflation factor (GIF) - a value around 1 means the test(s) is appropriately calibrated. Here it is 1.02, which I'm happy with.
oys_6.pv$gif
```

An appropriately calibrated set of tests will have a GIF of around 1. The elevated GIF for our tests indicates that the results may be overly liberal in identifying candidate SNPs. If the GIF is less than one, the test may be too conservative.

```{r}
# look at how application of the GIF to the p-values impacts the p-value distribution:
hist(oys_6.pv$pvalue[,1], main="Unadjusted p-values")        
hist(oys_6.pv$calibrated.pvalue[,1], main="GIF-adjusted p-values")
```
There should be a relatively flat histogram (most loci not under selection) with a peak near zero, indicative of candidate adaptive markers. The peak around 0 is more apparent in the adjusted p-value histogram.

```{r}
# Let's change the GIF and readjust the p-values:
zscore_6 <- oys_6.pv$score[,1]   # zscores for first predictor, we only have one in our case...
(gif_6 <- oys_6.pv$gif[1])       ## d.fault GIF for this predictor
```

```{r}
new.gif6 <- 1.00              ## choose your new GIF

# Manual adjustment of the p-values:
adj.pv6 <- pchisq(zscore_6^2/new.gif6, df=1, lower = FALSE)
```

```{r}
# plot the p-value histograms with the new gif
hist(oys_6.pv$pvalue[,1], main="Unadjusted p-values")        
hist(oys_6.pv$calibrated.pvalue[,1], main="GIF-adjusted p-values (GIF=1.02)")
hist(adj.pv6, main="REadjusted p-values (GIF=1.00)")
```

```{r}
#convert adjusted p-values to q values - q-values provide a measure of each SNP’s significance, automatically taking into account the fact that thousands are simultaneously being tested
# then an FDR threshold can be used to control the number of false positive detections
oys_6.qv <- qvalue(oys_6.pv$calibrated.pvalue)$qvalues

length(which(oys_6.qv < 0.1)) ## how many SNPs have an FDR < 10%?
```

```{r}
(oys_6.FDR.1 <- colnames(gen_6)[which(oys_6.qv < 0.1)]) ## i.entify which SNPs these are
```

```{r}
# Removing "V" from the snp name so it can be converted to an integer
oys_outlier_6pops <- oys_6.FDR.1%>%str_replace("V","")
```

```{r}
invisible(lapply(oys_outlier_6pops, write, "outliers_lfmm2_6pops.txt", append=TRUE))
```
